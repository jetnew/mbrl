{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import gym\n",
    "import torch\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc2690c512344a3bd000b87b018c272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "next_states = []\n",
    "for i in tqdm(range(100)):\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        states.append(obs)\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(obs)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2201, 4]),\n",
       " torch.Size([2201, 1]),\n",
       " torch.Size([2201, 1]),\n",
       " torch.Size([2201, 4]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t = torch.from_numpy(np.array(states).astype(np.float32))\n",
    "# a_t = torch.from_numpy(np.array(actions).astype(np.float32))  # for LunarLander-v0\n",
    "a_t = torch.unsqueeze(torch.from_numpy(np.array(actions).astype(np.float32)), 1)\n",
    "r_t = torch.unsqueeze(torch.from_numpy(np.array(rewards).astype(np.float32)), 1)\n",
    "s_t1 = torch.from_numpy(np.array(next_states).astype(np.float32))\n",
    "\n",
    "s_t.shape, a_t.shape, r_t.shape, s_t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "* Forward: $f(s_t,a_t)=s_{t+1}$\n",
    "* Backward: $f(s_{t+1}, a_t)=s_t$\n",
    "* Inverse: $f(s_t,s_{t+1})=a_t$\n",
    "* Reward: $f(s_{t+1})=r_{t+1}$, i.e. $f(s_t)=r_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(nn.Module):\n",
    "    \"\"\"Reward model predicts r_t1 from s_t1.\"\"\"\n",
    "    def __init__(self, s_dim):\n",
    "        super(Reward, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "    def forward(self, s_t1):\n",
    "        x = F.relu(self.fc1(s_t))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        r_hat_t1 = self.fc3(x)\n",
    "        return r_hat_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward(nn.Module):\n",
    "    \"\"\"Forward model predicts s_{t+1} from (s_t, a_t).\"\"\"\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_dim + a_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, s_dim)\n",
    "    def forward(self, s_t, a_t):\n",
    "        x = torch.cat((s_t, a_t), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        s_hat_t1 = self.fc3(x)\n",
    "        return s_hat_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backward(nn.Module):\n",
    "    \"\"\"Backward model predicts s_t from (s_{t+1}, a_t).\"\"\"\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Backward, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_dim + a_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, s_dim)\n",
    "    def forward(self, s_t1, a_t):\n",
    "        x = torch.cat((s_t1, a_t), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        s_hat_t = self.fc3(x)\n",
    "        return s_hat_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inverse(nn.Module):\n",
    "    \"\"\"Inverse model predicts a_t from (s_t, s_{t+1}).\"\"\"\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Inverse, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * s_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, a_dim)\n",
    "    def forward(self, s_t, s_t1):\n",
    "        x = torch.cat((s_t, s_t1), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        a_hat_t = self.fc3(x)\n",
    "        return a_hat_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_loss(s_t1, s_hat_t1):\n",
    "    return F.mse_loss(s_hat_t1, s_t1)\n",
    "\n",
    "def b_loss(s_t, s_hat_t):\n",
    "    return F.mse_loss(s_hat_t, s_t)\n",
    "\n",
    "def i_loss(a_t, a_hat_t):\n",
    "    return F.mse_loss(a_hat_t, a_t)\n",
    "\n",
    "def r_loss(r_t, r_hat_t):\n",
    "    return F.mse_loss(r_hat_t, r_t)\n",
    "\n",
    "def fbi_loss(s_t, s_hat_t, a_t, a_hat_t, s_t1, s_hat_t1):\n",
    "    return f_loss(s_t1, s_hat_t1) + b_loss(s_t, s_hat_t) + i_loss(a_t, a_hat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - F: 0.27, B: 0.24, I: 0.72, Reward: 0.87\n",
      "Epoch 1 - F: 0.27, B: 0.24, I: 0.71, Reward: 0.86\n",
      "Epoch 2 - F: 0.27, B: 0.24, I: 0.70, Reward: 0.84\n",
      "Epoch 3 - F: 0.27, B: 0.23, I: 0.70, Reward: 0.82\n",
      "Epoch 4 - F: 0.27, B: 0.23, I: 0.69, Reward: 0.80\n",
      "Epoch 5 - F: 0.26, B: 0.23, I: 0.68, Reward: 0.78\n",
      "Epoch 6 - F: 0.26, B: 0.23, I: 0.67, Reward: 0.76\n",
      "Epoch 7 - F: 0.26, B: 0.22, I: 0.66, Reward: 0.74\n",
      "Epoch 8 - F: 0.26, B: 0.22, I: 0.66, Reward: 0.72\n",
      "Epoch 9 - F: 0.26, B: 0.22, I: 0.65, Reward: 0.70\n",
      "Epoch 10 - F: 0.25, B: 0.22, I: 0.64, Reward: 0.68\n",
      "Epoch 11 - F: 0.25, B: 0.21, I: 0.63, Reward: 0.66\n",
      "Epoch 12 - F: 0.25, B: 0.21, I: 0.62, Reward: 0.64\n",
      "Epoch 13 - F: 0.25, B: 0.21, I: 0.61, Reward: 0.62\n",
      "Epoch 14 - F: 0.24, B: 0.21, I: 0.61, Reward: 0.60\n",
      "Epoch 15 - F: 0.24, B: 0.21, I: 0.60, Reward: 0.58\n",
      "Epoch 16 - F: 0.24, B: 0.20, I: 0.59, Reward: 0.56\n",
      "Epoch 17 - F: 0.24, B: 0.20, I: 0.58, Reward: 0.54\n",
      "Epoch 18 - F: 0.23, B: 0.20, I: 0.57, Reward: 0.52\n",
      "Epoch 19 - F: 0.23, B: 0.20, I: 0.56, Reward: 0.50\n",
      "Epoch 20 - F: 0.23, B: 0.20, I: 0.55, Reward: 0.48\n",
      "Epoch 21 - F: 0.23, B: 0.19, I: 0.55, Reward: 0.46\n",
      "Epoch 22 - F: 0.22, B: 0.19, I: 0.54, Reward: 0.44\n",
      "Epoch 23 - F: 0.22, B: 0.19, I: 0.53, Reward: 0.42\n",
      "Epoch 24 - F: 0.22, B: 0.19, I: 0.52, Reward: 0.40\n",
      "Epoch 25 - F: 0.21, B: 0.19, I: 0.51, Reward: 0.38\n",
      "Epoch 26 - F: 0.21, B: 0.19, I: 0.50, Reward: 0.36\n",
      "Epoch 27 - F: 0.21, B: 0.18, I: 0.49, Reward: 0.34\n",
      "Epoch 28 - F: 0.20, B: 0.18, I: 0.49, Reward: 0.32\n",
      "Epoch 29 - F: 0.20, B: 0.18, I: 0.48, Reward: 0.30\n",
      "Epoch 30 - F: 0.20, B: 0.18, I: 0.47, Reward: 0.29\n",
      "Epoch 31 - F: 0.19, B: 0.18, I: 0.46, Reward: 0.27\n",
      "Epoch 32 - F: 0.19, B: 0.17, I: 0.45, Reward: 0.25\n",
      "Epoch 33 - F: 0.19, B: 0.17, I: 0.44, Reward: 0.24\n",
      "Epoch 34 - F: 0.18, B: 0.17, I: 0.43, Reward: 0.22\n",
      "Epoch 35 - F: 0.18, B: 0.17, I: 0.43, Reward: 0.21\n",
      "Epoch 36 - F: 0.18, B: 0.16, I: 0.42, Reward: 0.19\n",
      "Epoch 37 - F: 0.17, B: 0.16, I: 0.41, Reward: 0.18\n",
      "Epoch 38 - F: 0.17, B: 0.16, I: 0.40, Reward: 0.17\n",
      "Epoch 39 - F: 0.16, B: 0.16, I: 0.39, Reward: 0.15\n",
      "Epoch 40 - F: 0.16, B: 0.15, I: 0.38, Reward: 0.14\n",
      "Epoch 41 - F: 0.16, B: 0.15, I: 0.38, Reward: 0.13\n",
      "Epoch 42 - F: 0.15, B: 0.15, I: 0.37, Reward: 0.12\n",
      "Epoch 43 - F: 0.15, B: 0.15, I: 0.36, Reward: 0.11\n",
      "Epoch 44 - F: 0.15, B: 0.14, I: 0.35, Reward: 0.10\n",
      "Epoch 45 - F: 0.14, B: 0.14, I: 0.35, Reward: 0.09\n",
      "Epoch 46 - F: 0.14, B: 0.14, I: 0.34, Reward: 0.09\n",
      "Epoch 47 - F: 0.14, B: 0.14, I: 0.33, Reward: 0.08\n",
      "Epoch 48 - F: 0.13, B: 0.13, I: 0.33, Reward: 0.07\n",
      "Epoch 49 - F: 0.13, B: 0.13, I: 0.32, Reward: 0.07\n",
      "Epoch 50 - F: 0.12, B: 0.13, I: 0.32, Reward: 0.06\n",
      "Epoch 51 - F: 0.12, B: 0.12, I: 0.31, Reward: 0.06\n",
      "Epoch 52 - F: 0.12, B: 0.12, I: 0.30, Reward: 0.06\n",
      "Epoch 53 - F: 0.12, B: 0.12, I: 0.30, Reward: 0.05\n",
      "Epoch 54 - F: 0.11, B: 0.11, I: 0.29, Reward: 0.05\n",
      "Epoch 55 - F: 0.11, B: 0.11, I: 0.29, Reward: 0.05\n",
      "Epoch 56 - F: 0.11, B: 0.11, I: 0.28, Reward: 0.05\n",
      "Epoch 57 - F: 0.10, B: 0.11, I: 0.28, Reward: 0.04\n",
      "Epoch 58 - F: 0.10, B: 0.10, I: 0.28, Reward: 0.04\n",
      "Epoch 59 - F: 0.10, B: 0.10, I: 0.27, Reward: 0.04\n",
      "Epoch 60 - F: 0.10, B: 0.10, I: 0.27, Reward: 0.04\n",
      "Epoch 61 - F: 0.09, B: 0.09, I: 0.27, Reward: 0.04\n",
      "Epoch 62 - F: 0.09, B: 0.09, I: 0.26, Reward: 0.04\n",
      "Epoch 63 - F: 0.09, B: 0.09, I: 0.26, Reward: 0.04\n",
      "Epoch 64 - F: 0.09, B: 0.08, I: 0.26, Reward: 0.04\n",
      "Epoch 65 - F: 0.08, B: 0.08, I: 0.25, Reward: 0.04\n",
      "Epoch 66 - F: 0.08, B: 0.08, I: 0.25, Reward: 0.04\n",
      "Epoch 67 - F: 0.08, B: 0.08, I: 0.25, Reward: 0.03\n",
      "Epoch 68 - F: 0.08, B: 0.07, I: 0.24, Reward: 0.03\n",
      "Epoch 69 - F: 0.08, B: 0.07, I: 0.24, Reward: 0.03\n",
      "Epoch 70 - F: 0.08, B: 0.07, I: 0.24, Reward: 0.03\n",
      "Epoch 71 - F: 0.07, B: 0.07, I: 0.23, Reward: 0.03\n",
      "Epoch 72 - F: 0.07, B: 0.06, I: 0.23, Reward: 0.03\n",
      "Epoch 73 - F: 0.07, B: 0.06, I: 0.23, Reward: 0.03\n",
      "Epoch 74 - F: 0.07, B: 0.06, I: 0.23, Reward: 0.03\n",
      "Epoch 75 - F: 0.07, B: 0.06, I: 0.22, Reward: 0.03\n",
      "Epoch 76 - F: 0.07, B: 0.05, I: 0.22, Reward: 0.03\n",
      "Epoch 77 - F: 0.06, B: 0.05, I: 0.22, Reward: 0.02\n",
      "Epoch 78 - F: 0.06, B: 0.05, I: 0.21, Reward: 0.02\n",
      "Epoch 79 - F: 0.06, B: 0.05, I: 0.21, Reward: 0.02\n",
      "Epoch 80 - F: 0.06, B: 0.05, I: 0.21, Reward: 0.02\n",
      "Epoch 81 - F: 0.06, B: 0.04, I: 0.20, Reward: 0.02\n",
      "Epoch 82 - F: 0.06, B: 0.04, I: 0.20, Reward: 0.02\n",
      "Epoch 83 - F: 0.06, B: 0.04, I: 0.20, Reward: 0.02\n",
      "Epoch 84 - F: 0.05, B: 0.04, I: 0.19, Reward: 0.02\n",
      "Epoch 85 - F: 0.05, B: 0.04, I: 0.19, Reward: 0.02\n",
      "Epoch 86 - F: 0.05, B: 0.04, I: 0.19, Reward: 0.02\n",
      "Epoch 87 - F: 0.05, B: 0.03, I: 0.18, Reward: 0.02\n",
      "Epoch 88 - F: 0.05, B: 0.03, I: 0.18, Reward: 0.02\n",
      "Epoch 89 - F: 0.05, B: 0.03, I: 0.18, Reward: 0.02\n",
      "Epoch 90 - F: 0.05, B: 0.03, I: 0.17, Reward: 0.02\n",
      "Epoch 91 - F: 0.04, B: 0.03, I: 0.17, Reward: 0.01\n",
      "Epoch 92 - F: 0.04, B: 0.03, I: 0.17, Reward: 0.01\n",
      "Epoch 93 - F: 0.04, B: 0.03, I: 0.16, Reward: 0.01\n",
      "Epoch 94 - F: 0.04, B: 0.03, I: 0.16, Reward: 0.01\n",
      "Epoch 95 - F: 0.04, B: 0.03, I: 0.16, Reward: 0.01\n",
      "Epoch 96 - F: 0.04, B: 0.02, I: 0.15, Reward: 0.01\n",
      "Epoch 97 - F: 0.04, B: 0.02, I: 0.15, Reward: 0.01\n",
      "Epoch 98 - F: 0.03, B: 0.02, I: 0.14, Reward: 0.01\n",
      "Epoch 99 - F: 0.03, B: 0.02, I: 0.14, Reward: 0.01\n"
     ]
    }
   ],
   "source": [
    "f_f = Forward(s_dim=4, a_dim=1)\n",
    "f_b = Backward(s_dim=4, a_dim=1)\n",
    "f_i = Inverse(s_dim=4, a_dim=1)\n",
    "f_r = Reward(s_dim=4)\n",
    "\n",
    "f_opt = Adam(f_f.parameters())\n",
    "b_opt = Adam(f_b.parameters())\n",
    "i_opt = Adam(f_i.parameters())\n",
    "r_opt = Adam(f_r.parameters())\n",
    "\n",
    "\n",
    "fbi_losses = []\n",
    "for i in range(100):\n",
    "    f_opt.zero_grad()\n",
    "    b_opt.zero_grad()\n",
    "    i_opt.zero_grad()\n",
    "    r_opt.zero_grad()\n",
    "    \n",
    "    s_hat_t1 = f_f(s_t, a_t)\n",
    "    s_hat_t = f_b(s_t1, a_t)\n",
    "    a_hat_t = f_i(s_t, s_t1)\n",
    "    r_hat_t = f_r(s_t1)\n",
    "    \n",
    "    \n",
    "    loss_f = f_loss(s_t1, s_hat_t1)\n",
    "    loss_b = b_loss(s_t, s_hat_t)\n",
    "    loss_i = i_loss(a_t, a_hat_t)\n",
    "    loss_r = r_loss(r_t, r_hat_t)\n",
    "    \n",
    "    loss_f.backward()\n",
    "    loss_b.backward()\n",
    "    loss_i.backward()\n",
    "    loss_r.backward()\n",
    "    \n",
    "    f_opt.step()\n",
    "    b_opt.step()\n",
    "    i_opt.step()\n",
    "    r_opt.step()\n",
    "    \n",
    "    print(f\"Epoch {i} - F: {loss_f:.2f}, B: {loss_b:.2f}, I: {loss_i:.2f}, Reward: {loss_r:.2f}\")\n",
    "    fbi_losses.append([loss_f, loss_b, loss_i, loss_r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Policy Optimization (MBPO)\n",
    "\n",
    "* Based on the Dyna algorithm\n",
    "1. Collect environment trajectories; add to $D_{env}$\n",
    "2. Train model ensemble on environment data $D_{env}$\n",
    "3. Perform k-step model rollouts branched from $D_{env}$; add to $D_{model}$\n",
    "4. Update policy parameters on model data $D_{model}$\n",
    "\n",
    "Source: [BAIR](https://bair.berkeley.edu/blog/2019/12/12/mbpo/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seungeunrho/minimalRL/blob/master/dqn.py\n",
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 32\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "      \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()\n",
    "            \n",
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :20, score : 10.2, n_buffer : 205, eps : 7.9%\n",
      "n_episode :40, score : 9.5, n_buffer : 395, eps : 7.8%\n",
      "n_episode :60, score : 9.8, n_buffer : 592, eps : 7.7%\n",
      "n_episode :80, score : 9.8, n_buffer : 787, eps : 7.6%\n",
      "n_episode :100, score : 9.7, n_buffer : 980, eps : 7.5%\n",
      "n_episode :120, score : 9.7, n_buffer : 1174, eps : 7.4%\n",
      "n_episode :140, score : 9.9, n_buffer : 1372, eps : 7.3%\n",
      "n_episode :160, score : 10.0, n_buffer : 1572, eps : 7.2%\n",
      "n_episode :180, score : 9.5, n_buffer : 1762, eps : 7.1%\n",
      "n_episode :200, score : 9.9, n_buffer : 1960, eps : 7.0%\n",
      "n_episode :220, score : 12.6, n_buffer : 2212, eps : 6.9%\n",
      "n_episode :240, score : 9.7, n_buffer : 2406, eps : 6.8%\n",
      "n_episode :260, score : 11.2, n_buffer : 2631, eps : 6.7%\n",
      "n_episode :280, score : 12.0, n_buffer : 2871, eps : 6.6%\n",
      "n_episode :300, score : 14.6, n_buffer : 3162, eps : 6.5%\n",
      "n_episode :320, score : 15.5, n_buffer : 3472, eps : 6.4%\n",
      "n_episode :340, score : 24.4, n_buffer : 3961, eps : 6.3%\n",
      "n_episode :360, score : 105.5, n_buffer : 6070, eps : 6.2%\n",
      "n_episode :380, score : 188.2, n_buffer : 9833, eps : 6.1%\n",
      "n_episode :400, score : 238.6, n_buffer : 14605, eps : 6.0%\n",
      "n_episode :420, score : 236.2, n_buffer : 19328, eps : 5.9%\n",
      "n_episode :440, score : 242.6, n_buffer : 24179, eps : 5.8%\n",
      "n_episode :460, score : 225.7, n_buffer : 28692, eps : 5.7%\n",
      "n_episode :480, score : 209.9, n_buffer : 32891, eps : 5.6%\n",
      "n_episode :500, score : 196.8, n_buffer : 36827, eps : 5.5%\n",
      "n_episode :520, score : 224.4, n_buffer : 41315, eps : 5.4%\n",
      "n_episode :540, score : 210.6, n_buffer : 45527, eps : 5.3%\n",
      "n_episode :560, score : 205.2, n_buffer : 49632, eps : 5.2%\n",
      "n_episode :580, score : 203.2, n_buffer : 50000, eps : 5.1%\n",
      "n_episode :600, score : 272.8, n_buffer : 50000, eps : 5.0%\n",
      "n_episode :620, score : 208.7, n_buffer : 50000, eps : 4.9%\n",
      "n_episode :640, score : 207.0, n_buffer : 50000, eps : 4.8%\n",
      "n_episode :660, score : 182.8, n_buffer : 50000, eps : 4.7%\n",
      "n_episode :680, score : 205.1, n_buffer : 50000, eps : 4.6%\n",
      "n_episode :700, score : 233.9, n_buffer : 50000, eps : 4.5%\n",
      "n_episode :720, score : 344.4, n_buffer : 50000, eps : 4.4%\n",
      "n_episode :740, score : 200.4, n_buffer : 50000, eps : 4.3%\n",
      "n_episode :760, score : 223.9, n_buffer : 50000, eps : 4.2%\n",
      "n_episode :780, score : 210.8, n_buffer : 50000, eps : 4.1%\n",
      "n_episode :800, score : 241.9, n_buffer : 50000, eps : 4.0%\n",
      "n_episode :820, score : 272.8, n_buffer : 50000, eps : 3.9%\n",
      "n_episode :840, score : 260.4, n_buffer : 50000, eps : 3.8%\n",
      "n_episode :860, score : 195.3, n_buffer : 50000, eps : 3.7%\n",
      "n_episode :880, score : 248.8, n_buffer : 50000, eps : 3.6%\n",
      "n_episode :900, score : 269.1, n_buffer : 50000, eps : 3.5%\n",
      "n_episode :920, score : 254.8, n_buffer : 50000, eps : 3.4%\n",
      "n_episode :940, score : 267.6, n_buffer : 50000, eps : 3.3%\n",
      "n_episode :960, score : 303.1, n_buffer : 50000, eps : 3.2%\n",
      "n_episode :980, score : 279.6, n_buffer : 50000, eps : 3.1%\n",
      "n_episode :1000, score : 309.1, n_buffer : 50000, eps : 3.0%\n",
      "n_episode :1020, score : 271.9, n_buffer : 50000, eps : 2.9%\n",
      "n_episode :1040, score : 186.5, n_buffer : 50000, eps : 2.8%\n",
      "n_episode :1060, score : 269.8, n_buffer : 50000, eps : 2.7%\n",
      "n_episode :1080, score : 300.9, n_buffer : 50000, eps : 2.6%\n",
      "n_episode :1100, score : 297.4, n_buffer : 50000, eps : 2.5%\n",
      "n_episode :1120, score : 281.1, n_buffer : 50000, eps : 2.4%\n",
      "n_episode :1140, score : 351.6, n_buffer : 50000, eps : 2.3%\n",
      "n_episode :1160, score : 235.8, n_buffer : 50000, eps : 2.2%\n",
      "n_episode :1180, score : 265.0, n_buffer : 50000, eps : 2.1%\n",
      "n_episode :1200, score : 318.1, n_buffer : 50000, eps : 2.0%\n",
      "n_episode :1220, score : 307.5, n_buffer : 50000, eps : 1.9%\n",
      "n_episode :1240, score : 331.8, n_buffer : 50000, eps : 1.8%\n",
      "n_episode :1260, score : 359.4, n_buffer : 50000, eps : 1.7%\n",
      "n_episode :1280, score : 325.8, n_buffer : 50000, eps : 1.6%\n",
      "n_episode :1300, score : 267.4, n_buffer : 50000, eps : 1.5%\n",
      "n_episode :1320, score : 305.2, n_buffer : 50000, eps : 1.4%\n",
      "n_episode :1340, score : 265.5, n_buffer : 50000, eps : 1.3%\n",
      "n_episode :1360, score : 303.2, n_buffer : 50000, eps : 1.2%\n",
      "n_episode :1380, score : 261.7, n_buffer : 50000, eps : 1.1%\n",
      "n_episode :1400, score : 260.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1420, score : 272.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1440, score : 188.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1460, score : 118.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1480, score : 153.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1500, score : 142.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1520, score : 181.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1540, score : 202.5, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1560, score : 137.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1580, score : 169.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1600, score : 128.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1620, score : 194.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1640, score : 283.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1660, score : 214.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1680, score : 256.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1700, score : 298.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1720, score : 260.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1740, score : 304.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1760, score : 260.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1780, score : 254.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1800, score : 281.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1820, score : 333.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1840, score : 308.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1860, score : 270.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1880, score : 312.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1900, score : 257.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1920, score : 196.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1940, score : 263.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1960, score : 251.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1980, score : 256.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2000, score : 273.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2020, score : 244.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2040, score : 303.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2060, score : 243.2, n_buffer : 50000, eps : 1.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ae919d742166>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_epi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mprint_interval\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn_epi\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a6e729054d73>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(q, q_target, memory, optimizer)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mq_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mq_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mmax_q_prime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\notes\\Anaconda3\\envs\\mbrl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a6e729054d73>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\notes\\Anaconda3\\envs\\mbrl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\notes\\Anaconda3\\envs\\mbrl\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\notes\\Anaconda3\\envs\\mbrl\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# config\n",
    "episodes = 10_000\n",
    "print_interval = 20\n",
    "\n",
    "# model\n",
    "env = gym.make('CartPole-v1')\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for e in range(episodes):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if e % print_interval == 0 and e != 0:\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(f\"Episode: {e}, Score: {score/print_interval:.1f}, Buffer size: {memory.size()}, Epsilon : {epsilon*100:.1f}%\")\n",
    "        score = 0.0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mbrl)",
   "language": "python",
   "name": "mbrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
